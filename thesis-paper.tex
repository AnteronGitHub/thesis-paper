\documentclass[officiallayout]{tktla}
%\documentclass[officiallayout,a4frame]{tktla}
\usepackage[latin1]{inputenc}
\usepackage{latexsym}
\usepackage{graphicx}

\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{bytefield}
\usepackage{graphicx}
\usepackage{subcaption}

\newcommand{\pluseq}{\mathrel{+}=}

\title{Automated Software Configuration for Cloud Deployment}
\author{Antero Vainio}
\authorcontact{antero.vainio@helsinki.fi}
\pubtime{September}{2020}
\reportno{}
\isbnpaperback{}
\isbnpdf{}
\issn{}
\printhouse{}
\pubpages{7} % --- remember to update this!
% For monographs, the number of the last page of the list of references
% For article-based theses, the number of the last page of the list of
% references of the preamble part + the total number of the pages of
% the original articles and interleaf pages.
\supervisorlist{Lirim Osmani, Ashwin Rao, University of Helsinki, Finland}
\preexaminera{}
\preexaminerb{}
\custos{}
\generalterms{thesis, example, another example, still more examples,
  more and more examples}
\additionalkeywords{example, an example phrase with many words}
% Computing Reviews 1998 style
%\crcshort{A.0, C.0.0}
%\crclong{
%\item[A.0] Example Category
%\item[C.0.0] Another Example
%}
% Computing Reviews 2012 style
\crclong{
\item Example Category $\rightarrow$ And its subcategory $\rightarrow$ And sub-subcategory
\item Another Example  $\rightarrow$ And its subcategory
}
\permissionnotice{
    Thesis paper, to be published.
}

\newtheorem{theorem}{Theorem}[chapter]
\newenvironment{proof}{\noindent\textbf{Proof.} }{$\Box$}

\begin{document}

\frontmatter

\maketitle

\tableofcontents

\mainmatter

\chapter{Introduction}

As providing services over the Internet has become a common practice in the
modern society, maintenance of information systems has encountered many
challenges as well. While web-based applications have become more diverse,
resource-intensive, and complicated, they face higher expectations with respect
to usability, performance and security.

Often times industrial software development has strict deadlines to follow.
Many development practices are built around the idea of constant service
improvement. It means that a software product is rarely considered finished but
instead new features are being added to it and flaws being fixed in a priority
order.

As a result, programs and sometimes entire system architectures tend to require
frequent updates. Similarly to IT services automating repetitive tasks in hopes
of achieving reliability and cost-effectiveness, software development processes
aim to utilize automation whenever feasible.

Cloud enviroments offer on-demand computing resources for cloud consumers. When
deploying to cloud, a user can expect customized servers being provisioned
within seconds. It can be achieved with a combination of hypervisors and
programs preinstalled in virtual machines for instance. For further
configurations, additional software automation tools can be used.

Cloud environments are formed by physical host machines that lease virtualized
devices such as virtual CPUs (vCPU), logical volumes, or virtual network
devices. Users get typically charged for the time that they use some of these
resources. Servers can quickly be set up and down, preventing users from
having to pay for under-utilized servers.

For server provisioning, cloud providers do not typically have the same
luxuries as cloud users, since using virtualization is not optimal due to added
processing overhead. However when it comes to configuring hundreds or even
thousands of physical server computers, automation is once again seen as a
potential solution. Yet many traditional cloud administrators are still relying
on manual maintenance operations.

This thesis explores different popular methods for deploying and administering
cloud environments with help of software automation. Selected automation tools
and de facto methods for using them are compared. Questions \ref{fig:rqs} are
reflected during evaluation.

Deployed software is a combination OpenStack services. OpenStack is a
collective of open-source software projects for running cloud servers.
OpenStack is the most widely used plaftorm for creating private clouds. It
includes official repositories for various automated deployment methods. Most
of the methods provide a basis on which to build a solution customized for own
working environment.

\begin{figure}[t]
\centering
\begin{itemize}
  \item [RQ1] What are the key factors affecting the design of different
              deployment methods, and how do they differentiate from each
              other?
  \item [RQ2] What kind of features do different deployment methods offer?
              Which components are used in deployments?
  \item [RQ3] Who will benefit from the development of software automation
              tools, and why?
\end{itemize}
\caption{Research Questions}
\label{fig:rqs}
\end{figure}

\chapter{State Of The Art}

This thesis is motivated by current trends and contemporary practices used for
administering IT infrastructures. Cloud computing has reached nearly a de facto
status for large scale infrastructure provisioning. While cloud services
automate many administrative tasks, another increasingly utilized method in IT
service management is the use of software automation for system configurations.

Usually whenever there is a trend or a commonly followed practice in the field
of computer science, there are various approaches taken to achieve it.
Different approaches may share similarities in some aspects while at the same
time they can be fundamentally different in other aspects. One of the reasons
for the existence of various alternative solutions can be commercial
competition, another can bedifferent preferences for software architectures.

There is a lot of diversity when it comes to the use of software solutions for
IT service management. However for the most parts IT services themselves have
similar requirements related to the quality of service. Services have
requirements such as highly availability, reliability, efficiency, usability,
and security. At the same time service management needs to be cost-efficient.
When it comes to providing high quality with low cost, proper use of automated
solutions may prove to be valuable.

With successful Internet based services, scale makes a big difference. Spikes
in the number of active users can affect the quality of service significantly,
even to the point where the service is unusable. At the same time, having
under-utilized servers wastes resources and thus increases the cost of
providing service, resulting in competitive disadvantage in the market. This
must be considered both in the design of the application software and in the
infrastructure architecture.

Number of users for a web-based application can change a lot during the course
of a regular business day. Many services have repeating patterns in their
level of usage and the number of users for a particular time of day can be
predicted with high accuracy. If usage level has drastic changes and follows a
repeating pattern, provisioned computing resources can be scaled correspondinly
resulting in cost savings for optimized resource utilization. With the help of
detailed analysis and the use of cloud platforms, this can be achieved.

When scaling infrastructure, many administrative tasks become repetitive. When
scaling up, deployed applications need to be configured and integrated to
environment. When scaling down, it has to be ensured that no application is
requesting removed application instances. When done manually these tasks have a
high risk of failure due to human error to the point where they cannot be
expected to be frequently without this adding a lot of costs.

Even when infrastructure is not scaled rapidly, there are repeating
administrative tasks, such as software updates and hardware maintentance
operations. Having to assure service operations during maintenance suffers the
same risk of human error, when relying on manual operations. Still this is a
practice for many administrators and may even limit the potential of the
provided IT service.

Cloud computing can be used for provisioning computing resources rapidly. For
customized applications, additional configuration is often necessary. Software
configuration management is a potential solution for avoiding human error in
repetitive configuration tasks as it enables these configurations to be applied
programmatically. While providing operational reliability, it has additional
benefits such as fast execution, and documentation value provided formally
defined configuration tasks.

Another recent trend in IT service management is the DevOps movement. The term
DevOps is often used to describe a principle of speeding up development by
simplifying the process of publishing new software versions. By using efficient
development processes and automated tools, the time it takes to
deploy a new software version to runtime environment or package distribution
can be cut, lowering the overall cost of IT development. DevOps movement was
motivated by agile software development principles and the realization that
communication between software developers and operators often times ended up
being an unnecessary bottleneck for project speed.

Some of the goals of DevOps movement, such as fast and automated software
release process, can be achieved with a proper use of cloud platforms and
software configuration management tools. They allow application developers to
define the deployment process either completely or partially. Deployment is
executed by running applications as opposed to manually following with
application specific instructions. Use of automation avoids errors resulting
from miscommunication between developers and operators.

This chapter provides a short introduction to the concepts behind cloud
computing and software confuration management. Some contemporary software
solutions for these concepts. Descriptions aim to provide answers to research
question RQ1 shown on Figure \ref{fig:rqs}. Key design principles and
differences of the tools are presented as they will affect any deployment
methods related to them.

\section{Cloud Computing}

For the last decade, cloud computing has been a common paradigm for IT
infrastructure management. It provides benefits such as high server utilization
and dynamic scalability with a pay-as-you-go price model for outsourced server
infrastructure. Cloud computing is being widely used in various industrial
fields, including telecommunication, retail, finance, and scientific research.
As a result, there is also an inreasing number of public cloud platforms
available, most notably Amazon Web Services (AWS) \cite{aws}, Microsoft Azure
\cite{azure}, and Google Cloud Platform (GCP) \cite{gcp}.

As with many practices in the field of computer science, cloud computing lacks
a universally agreed formal definition. However common to most of the
contemporary cloud platforms is the ability for the end user to provision
computing resources over HTTP interfaces, mostly by using Restful APIs. This
makes it possible to automate infrastructure provisioning. Cloud consumer
avoids the need to request infrastructure operations from administrators, while
administrators don't have to spend time for repetitive tasks related to
infrastructure provisioning.

Due to its on-demand nature, a term often used to describe cloud resource
provisioning model is \textit{Infrastructure-as-a-Service} (IaaS). It
emphasizes the fact that infrastructure can be provisioned through a
well-defined interface. IaaS can be generalized to term
\textit{Anything-as-a-Service} (XaaS), where anything stands for any resource
that is provided through programmable interfaces. Whereas in IaaS, the provided
resources are relatively low-level computing resources, such as servers,
networks or volumes, in \textit{Software-as-a-Service} model, in addition to
service provider provisioning infrastructure, it takes care of software
licencing and configuration.

Reason for varying service models offered by cloud providers, is the fact that
clients have varying needs in terms of outsourced software solutions. Some
clients may want an easily adaptable software configuration with no need for
additional configurations and expertice related to the software; in case of
issues with the software they are willing to pay for customer support. At the
same time there are clients who use cloud providers only for leasing
infrastructure in order to avoid having to maintain own data centers. For the
first type of client, SaaS may be the right service model, and the latter might
prefer IaaS.

When it comes to providing XaaS model services, cloud service providers have
advantages for making new products. As they know their cloud platform in
detail, they are able to produce optimized solutions for particular use cases.
They also have full access to their platform, making it possible to avoid any
workaround solutions that may arise from having to tailor solutions to
interfaces provided by other services.

Reasons why some clients may still prefer to configure software being hosted on
cloud platforms themselves, are saves in costs, and the ability to tailor
solutions to their particular needs. XaaS products have to be generalized
enough so that they can be used by a large customer base. The more a product is
generalized, the less it can be directly adapted to a particular use case.
There are different approaches to dealing with this dilemma, such as
configuration options or large catalogs of tailored services.

Another characteristic in cloud environments is seemingly limitless resources
available on-demand, enabling the infrastructure to be scaled rapidly based on
current usage. As cloud platforms can be used for various computational tasks,
investing in large data centers is more reliable for cloud providers than for
any particular industrial field practicioner.

With more computing resources available, and the ability to provision them for
an execution of a particular computational task with no extra cost, some cloud
consumers may use the platform to lease many vCPU hours for a short period of
time, instead of using less vCPU hours for a longer period of time. This way of
using cloud platform is more typical with computational applications such as
with business analytics, and scientific research.

Ability to think of a cloud platform as having limitless computing resources
makes it possible to consider cloud platforms alternatives for \textit{High
Performance Computing} (HPC) or \textit{High Throughput Computing} (HTC). In
general this would be achieved by scaling computations horizontally. If a heavy
computational task can be split to subtasks that can be run in parallel, they
can be executed on different hardware, shortening the overall time of
computation. Similarly, if a large data stream can be split to smaller streams
to be combined at receiver, the overall time of transmission can be shortened
by using different network links. MapReduce \cite{mapreduce} is an application
developed to help parallelize computational tasks by defining them as map and
reduce fuctions. This model makes it possible to parallelize and thus scale
computations horizontally by the runtime system.

Although cloud platforms primarily provide scalability horizontally with a
number of provisioned instances, some of the larger cloud platforms provide
also instance flavors with highly performant computing hardware. These flavors
may be provided for bare metal services to reduce computational overhead of
virtualization. While vertical scaling is not a feature distinct to cloud
platforms, they benefit from the high user base. This way while the platform
grows, as more varying computing flavors are being provided, computers with
high-quality CPUs and GPUs can be aquired in hopes that they will not go
unutilized.

Cloud environments may be public or private. While public and private cloud
services do not functionally differ from one another, private cloud services
are restricted to particular end users and public cloud services are offered
more or less without restrictions. As a result, generally public cloud
environments are larger in size, known examples of such environments include
AWS, Azure and GCP.

Some cloud platforms are built on top of a number of other cloud platforms.
Such architectures are called \textit{multi clouds}. Multi clouds may use a
particular cloud provider for certain types of resources, such as virtual
networks to be able to use quality wide area network infrastructure. They may
also use other cloud platforms for provisioning resources that they do not have
available at the moment.

\subsection{OpenStack}

OpenStack \cite{openstack} is the most widely used open-source software for
building cloud environments. It was originally developed by NASA and Rackspace,
and published openly in the year 2010. Since then it has been developed by
various organizations, including Cern nuclear research institute as well as
individual contributors. It has been battle-proven for a reliable cloud
operation \cite{forrester}. While OpenStack can be used for public clouds, it
is most commonly used for private clouds or multi clouds. It is currently being
also developed to provide support for edge computing infrastructures.

At its core OpenStack is an example of a IaaS platform. It provides
functionality for infrastructure provisioning through various services.
Infrastructure provisioning is also the primary use case of OpenStack for most
of its users. Many of the OpenStack services can be extended for providing
XaaS, such as VPN-as-a-Service or Database-as-a-Service. Some of these
extensions can be installed as plugins for the appropriate OpenStack services.
Otherwise any extensions have to be implemented.

In addition to installing OpenStack with low-level open-source projects,
OpenStack community has an official marketplace for commercial high-level
OpenStack-based software products. Commercial OpenStack products may provide
official integration to other platforms, or simplify and provide support for
the installation of OpenStack.

OpenStack consists of different services, not all of which are required for an
operational cloud. OpenStack services may also be used as a part of other
distributed systems without deploying a full OpenStack cloud. At basic level,
OpenStack services are Python application servers and related worker agents.
They use other infrastructure services, such as message queues, databases,
caches and proxies.

In order to grant access to resources, OpenStack services need to be able to
authenticate and authorize requestors, which may be end users or other
services. Amongst OpenStack services, Keystone provides Identity and Access
Management in cloud. Since OpenStack services use it for access control when
communicating with other services it is the first service to be installed in
an OpenStack deployment.

An essential resource in any computing environment is the actual computing
units. Traditionally OpenStack has provided computing resources by using
hypervisors, such as ESXI or KVM/QEMU. OpenStack compute service is called
Nova and it was one of the first services developed. Nova receives requests
that describe the specifications for needed virtual machines. Nova then
schedules the creation of the virtual machine from the hypervisor.

In order to access virtual machines created by Nova, network access is
required. Virtual network devices are used for on-demand network provisioning.
Originally virtual networks were provided by Nova service, but currenlty there
is a specific OpenStack service, called Neutron, that is responsible for
network provisioning. Even though nova-network is considered legacy, older
OpenStack deployments are still using it as they have not been updated to
support Neutron. As an example, Cern which has been a long-time OpenStack user,
is still using nova-network in its deployment.

Virtual machine images used by Nova are provided by Glance. Typically guest
operating system images are built by cloud administrator by using tools such as
disk-image-builder. However users are able to create virtual machine images
from their active Nova server instances.

OpenStack has a few services for persisting guest data. Service providing block
storage is Cinder. It provides mountable logical volumes for guest instances.
When instance is terminated, data stored in the volume will persist. Another
available service for persistent data is Swift object storage. It processes
stored data using object model. Lastly Manila is an OpenStack service that
provides shared file system.

There are some OpenStack services that are not required for a functional cloud,
but are almost always included in cloud deployment. One such service is
Horizon, a web UI for managing cloud projects. Users can login to it with their
credentials and do most of the operations available through other OpenStack
services.

Another optional but commonly deployed OpenStack service is Heat orchestration
engine. Heat takes a stack template file as an input and creates the resources
described in the file. This makes instansiating complex cloud stacks quick and
repeatable. In addition to creation, Heat has functionality for updating
deployed stacks based on changes made to its template file. Heat provides a
service similar to that of CloudFormation on AWS and it actually supports its
syntax. Ability to create system infrastructures with input files that are
parsed by computer programs is sometimes referred to as
\textit{Infrastructure-as-a-Code} (IaaC).

OpenStack deployments used for providing cloud services have to be designed
properly. Quality requirements related to availability, efficiency, security,
and any potential service level agreements have to be taken into consideration
when designing a cloud. Since cloud networking is more complicated than regular
data center networking, decent knowledge of the underlying technologies have to
be assured for both the designers and the administrators.

OpenStack Deployments typically include host machines of a few different
flavor. Typically host machines are classified to compute, storage, networking,
and controller nodes. Each of the node types includes only OpenStack services
that are used for corresponding functionality. Appropriate hardware based on
its use purpose should be installed on a node machine. Node  classification may
be different and miss miss some types or include other types depending on the
use case of the cloud. Host flavors have an important role when designing a
cloud architecture as it affects the cloud performance, power consumption and
overall cost.

Compute nodes are used for housing guest virtual machines. They provide guests
with CPU cores, RAM, root file system, and network access. The more CPU cores,
and RAM the node has the more guests it can run simultaneously. It is possible
to overcommit vCPUs with a defined ratio. Overcommitting increases the capacity
of CPU cores with a tradeoff efficiency in guests. For a compute node in a
production deployment having 16 CPU cores would be a feasible amount.

Compute nodes should have available RAM in a proportionally to available cores.
Guest instances are created from instance flavors that specify their resource
use. Flavors in the deployed cloud can provide guidelines to the compute node
hardware specifications.

Guest root volumes may be housed in shared file system in a network or in a
node, or they may be provided by directly mounting on the compute node file
system. In case the compute node file system is being mounted directly, compute
nodes need to have available volume capacity when creating new guest instances.
While using network volumes for guest root volumes may provide better volume
utilization, it suffers from slower read and write operations. Other options
related to volume provisioning is the device technology such as choice between
tape or SSD storage.

Storage nodes provide data persistence in the cloud. There are various
alternative backends for distributed storage accross nodes, such as Ceph,
GlusterFS and NFS. Hardware used for storage nodes should be oriented in volume
devices.

\subsection{Docker}

Docker \cite{docker} is a software for managing containerized applications.
Containers provide process isolation by using Linux Kernel cgroup \cite{cgroup}
functionality. Use of containers simplifies management of multiple
microservices but adds overhead and security considerations.

Docker containers use Docker Engine to communicate directly with host operating
system kernel. This makes containers more lightweight than virtual machines run
by hypervisors since kernel functionality does not have to be emulated by the
hypervisor.

In addition to providing runtime isolation for application processes, Docker
includes functionality for creating container images. Images can be created
from existing containers or based on files describing image configuration
tasks. Created images can be published to Docker \textit{registries}, which can
be public or private. DockerHub is a public container registry which houses
official Docker images, and is also available for other publishers.

Docker's image build system is sophisticated and provides multi-layered caching
for created images. Image is cached after distinct operations in the creation
process. By running configurations in the proper order only a subset of them
have to be repeated when image is modified and re-created. For instance by
updating base image package manager as the first configuration operation and
installing application dependencies later, updated package manager can be used
if application dependencies need to be reinstalled.

Docker also includes functionality for managing resources other than container.
Docker Engine can provision volumes for persisting data and networks for
container connectivity. There are different drivers for volumes and networks.
Simplest drivers simply bind resources directly to Docker host. For scaling
Dockerized application deployments overlay networks can be created for
containers so that they can connect to containers running on different hosts by
using layer 2 semantics.

One of the reasons for Docker becoming popular among software developers is how
it simplifies container creation and sharing container images. Since container
are portable to other hosts running Docker Engine, applications can be deployed
easily, while having any software dependencies included in built images makes
deployment quicker and more reliable. Lightweight container lifecycle also
enables containers to be recreated as a method of disaster recovery.

Docker's benefits for application deployment are ideal for teams adapting
DevOps principles. Not only do image registries provide an interface between
developer and operator teams, they make development environment configuration
simple enough to be appealing for application developers. Continuous deployment
pipelines benefit from Docker's build system.

By installing all the used software into containers, developer can keep the
host operating system environment uncluttered. Uninstalling software and its
files is simply done by removing the container.

Similarly to application developers benefitting from uncluttered host system,
operators running applications in containers do not have to worry about
cleaning thrash when updating outdated software versions. Containers provide
logistics for managing software that updates frequently. Running various
applications on a single host becomes also less complicated with containers.

\subsection{Kubernetes}

Kubernetes \cite{kubernetes} is a software for managing Docker container
clusters. It was developed by Google which has been running applications in LXC
containers long before Docker became popular. Kubernetes is intended for
providing highly available production environments by running Docker container.
Due to container overlay networks it can be used for both centralized cloud
platforms, and de-centralised edge infrastructures.

Kubernetes uses Helm package manager to deploy applications from templates
called charts. While applications can be deployed in a Kubernetes cluster
without Helm packages, having Helm charts helps to bundle applications.

Kubernetes consists of api server, scheduler, cluster controller and worker
nodes. Clusters may provision resources from cloud providers by using separate
cloud controller process. In order to optimize Kubernetes cluster runtime for
production environments, baremetal Kubernetes clusters may be run. In this case
cloud controller is not necessary.

Kubernetes uses Docker Engine for managing containers. It can scale application
deployments by creating and deleting containers, and it provides health
monitoring and is capable of automatically recreating containers that have
entered failed state.

Other infrastructure can also be managed by Kubernetes. It manages firewall
rules within networks, and provides ingress to application containers through
reverse proxy.

Kubernetes is specifically useful for developers since application stacks can
be created faster than virtual machines. For administrators Kubernetes provides
functionality for deploying, updating and scaling applications. As a tradeoff,
having to install and operate Kubernetes cluster adds overhead to maintaining
applications.

There are commercial Kubernetes services, that offer configured clusters for
application developers. This makes it easy to operate scalable web-based
applications without the need to administer or configure host systems, which is
favourable for teams consisting only of software developers. On the other hand
administers may provide hosted Kubernetes clusters as a service.

OpenStack includes a service called Magnum, which can be used for providing
Kubernetes clusters as a service. Magnum uses Heat orchestration for
configuring cluster hosts that are created with Fedora Atomic machine images,
which include preconfigured Kubernetes software. Similarly to other OpenStack
services further customizations to Magnum are also possible.

In addition to Kubernetes being available to be provisioned as a service, there
also exists methods to deploy OpenStack services into Kubernetes clusters.
Since virtualization allows various possible stacks to be built from the same
components, it may sometimes create confusion. How different stacks are built
depends on the intended use cases, but for the most parts, the more
virtualization stack is had, the more overhead is created, and thus it should
be avoided for production-grade environments.

\section{Software Configuration Management}

Large distributed systems, such as OpenStack deployments, consist of many
configuration items, such as databases, message queues, proxies, and memory
caches. Infrastructure services need to be configured according to applications
using them. As infrastructure services, or the application services, are
updated, compliancy to other services must also be ensured.

Shell scripting has traditionally been a common way of automating different
administrative tasks in Linux environment with its counterpart being batch
scripting in Windows. Recently different software automation tools have become
a popular replacement for scripting.

Automation tools commontly take an input file that describes operations to be
executed, and apply it. Input files are often given in declarative format, like
JSON \cite{json} or YAML \cite{yaml}, as opposed to procedural format of script
files. The key difference is, that input files describe the desired outcome and
not the methods of achieving it. How configuration is applied to the targeted
system is determined by the automation tool implementation. Since the methods
of configuring the system are for the most part not relevant to administrators,
using declarative input files simplifies automating software configuration
management.

Automation tools provide reliability and cross-platform support by using
application layer modules for software configuration tasks. They can abstract
operating system dependent considerations for common administrative tasks. This
also enables configurations to be shared more easily among administrators.

Typical recommendation when designing automated configuration tasks is to aim
for idempotent operations. This means that the task automated can be repeated
indefinitely without it changing the outcome. In other words, if the targeted
system is in the desired state, applying the configuration does not change the
target system state. Idempotence can be assured at low level by the automation
tool, but when making high level configurations, it has to be taken into
account by the configuration designer. Having idempotent configuration tasks
makes the use of automation tool more reliable.

One crucial difference in the use of software configuration management tools
and precreated virtual machine or container images, is that configuration
management tools deal with running configurations, while in created images the
necessary configurations have already been applied. It is possible to use
software configuration management tools for creating machine images, but most
of its usefulness comes from dynamic maintenance operations.

Dynamic configurations provide versatility and the ability to request newest
package versions over a network during execution. Tradeoff is that the
configuration takes longer than having preconfigured software available at
disk. In repeating configuration operations, such as CI/CD pipelines, virtual
machines containing some of the required software could be useful. Another
option is to use cache mechanism for installed dependencies. Optimizing build
time is usually possible in many ways once the pipeline is functional.

There is a large number of different software automation tools available. While
there are differences in the architecture of different software configuration
management tools, some similarities exist as well. Most of the software
configuration management tools provide some mechanism for sharing configuration
methods. This makes it possible to create communities of administrators.
Similarly to open-source software projects, configuration tool communities
often times help to develop the tool as well. As with many software projects,
many configuration management tools depend on active communities in order to
keep being developed. Some of the more relevant tools for this thesis are
introduced below in more detail.

\subsection{Ansible}

Ansible \cite{ansible} is a software management tool developed by RedHat with
community contributions. Ansible is an open-source program written in Python.
Ansible can be run with ad-hoc commands, without written input files, but
generally it is used with YAML formatted files describing operations to be
executed.

Use of Ansible is based on establishing SSH connection to target machine and
running Python modules executing administrative tasks. Hence it does not
require additional  agent programs installed on target machines, in addition to
SSH daemon and Python, both of which are available in many basic Linux distro
installations. Not requiring additional software makes the use of Ansible
simple and keeps target machines optimized.

Architecturally Ansible consists of a few concepts that divide the
responsibility of configuration execution. At lowest level, tasks are executed
in \textit{modules}, which are Python scripts that generally use standard
library API's for interacting with host operating system. For the most part the
modules need not be written when using Ansible, since Ansible standard library
includes modules for the most common administrative tasks. One of the standard
modules enables user to execute arbitrary shell commands, avoiding the need to
write a module for programs that do not have one implemented.

\begin{figure}[t]
\centering
\begin{verbatim}
---
- name: Copy files to destination
  copy:
    src: "files/{{ item }}"
    dest: "/opt/{{ item }}"
  with_items:
    - file.txt
    - other_file.txt
\end{verbatim}

\caption{Example Ansible task definition}
\label{fig:ansible-task}
\end{figure}

Ansible input files reference modules through units, called \textit{tasks}.
At minimal, tasks describe the module to be executed and arguments passed to
it. Tasks may include additional metadata, such as descriptive names that are
displayed during execution. Variables can be used with defined tasks in order
to make them re-usable. For instance, arguments passed to a module could be
defined with variables.

Figure \ref{fig:ansible-task} illustrates an example of Ansible task, which
uses \textit{files} module to copy files from \textit{files} directory to the
\textit{opt} directory on targeted host. It loops through file names specified
in \textit{with\_items} list, places the file names in the \textit{item}
placeholder in the task arguments.

Complex software configurations consist of many tasks. In order to structure
templates, Ansible uses \textit{roles}, that describe higher level
administrative operations. Roles are grouped in directories that follow a
conventional structure that Ansible expects. In order to roles to make changes
to target machines, they must include some tasks. Roles cannot be executed
directly, but rather have to be referenced externally. They are however
supposed to be kept self-contained, in order to port them easily. For this
reason roles can define default variable values to be used in tasks.

Input files that Ansible can execute are called \textit{playbooks}. They
describe hosts to be targeted, and reference roles to be applied or tasks to be
executed.

Environment specific information, such as host IP addresses, and customizable
configuration parameters are described in \textit{inventories}. Inventories can
be built from groups of hosts, and variable values can be specified for
individual hosts or for host groups.

While Ansible is easy to get started with, and provides modular a method for
building configuration libraries, it does not provide high-level in-built
functionality, such as health monitoring or infrastructure provisioning. These
tasks can be implemented with Ansible, and shared as roles for instance.

\subsection{Juju}

Juju \cite{juju} is an application modeling tool developed by Canonical. It is
capable of deploying, configuring and scaling software. Juju was originally
written it Python, but its current version is implemented with Go programming
language. It still has an API for Python.

Juju uses a \textit{cloud} abstraction for provisioning infrastructure.
Out-of-the-box it supports public clouds like AWS, GCP, and Azure, and a number
of private clouds, including OpenStack. For production deployments, Canonical
recommends using MAAS \cite{maas}, which can be installed in datacenter for
provisioning bare metal infrastructure. Juju also supports manual clouds for
deploying in pre-existing server infrastructure. Manual clouds lack some of the
functionality available when using other clouds, mainly related to automated
provisioning.

Juju uses a concept of \textit{application} for managing deployed software. All
of the components, including infrastructure services needed by deployed
software are, called applications by Juju. Operations related to applications,
such as infrastructure provisioning, installation and scaling, are executed by
Juju by running software packages, called \textit{charms}. Execution of charms
is triggered by administrator running commands on Juju client.

When deploying applications, Juju creates a special management node called
\textit{controller}. Controller maintains a database including data used by
\textit{models}. Models manage environment specific information of deployed
components, such as applications, storage volumes and network spaces. Hence
models provide an interface between application model, and its implementation
in cloud being used. Models additionally control access to infrastructure.

Juju is especially useful for application developers as it provides a simple
set of commands for operating application deployment and supports many of the
common infrastructure providers. Configuration tasks are implemented with
software modules using software libraries, making it more approachable for
developers familiar with the tools.

Administrators might find tools that provide declarative input format more
approachable. In case all of the configurations are provided by application
developers, Juju could provide an interface between developers and
administrators.

\subsection{Puppet}

Puppet \cite{puppet} is a Ruby based configuration management tool. Similarly
to other presented tools, configuration tasks are grouped in modules. Puppet
has open-source and commercial versions. Commercial version, called Puppet
Enterprise (PE), simplifies large-scale configuration management by providing
grahical user interface.

Puppet runs \textit{agent processes} on target machines to keep them in desired
state. Desired state can be described with Puppet's \textit{Domain Specific
Language (DSL)}, which is a declarative coding language. Puppet deployment
includes a \textit{master server}, which stores desired states in database
called \textit{PuppetDB}. Agent processes translate Puppet DSL into executable
commands. Master and agents use HTTPS protocol for communication \cite{puppet}.

Information about target hosts is gathered by agent processes with Puppets
inventory tool, \textit{Facter}. Gathered data is sent to master server in
Puppet DSL format in files called \textit{manifests}. Based on received
manifests, master server compiles JSON files, called \textit{catalogs} that
describe the desired state to agent processes. Puppet separates configuration
data from the code executing configurations by using tool called
\textit{Hiera}. Separating code from data makes modules more testable
\cite{puppet}.

Puppet provides accurate configuration monitoring by using agent processes on
target hosts. At the same time it includes many configuration items making the
installation process more complex. Overhead of configuration tool has a
potential of vendor-lock and means that the tools must provide value worth the
added work.

\chapter{OpenStack Deployment}\label{deployment}

Installation of a functional OpenStack cloud includes many steps. Most of the
OpenStack services use infrastructure services, that will need to be installed,
and configured for OpenStack services by managing appropriate resources and
user permissions.

As new versions of OpenStack services are release biannually, configuration
management is a big part operating an OpenStack cloud. After initial deployment
services will need to be upgraded, while ensuring cloud operation. In practice
the initial deployment covers only a small fraction of the OpenStack cloud
lifecycle. Often times however the tools used for initial deployment are also
used for many of the administrative tasks during cloud operation.

For the most part, OpenStack is beneficial for large-scale deployments. Many of
the infrastructure services aim to provide scalability for OpenStack services.
Using them for small-scale deployments is not beneficial as they consume
computing resources on hosts. Small scale data centers do not benefit from
RESTful APIs for infrastructure provisioning as much either.

Largest OpenStack deployments consist of thousands of nodes in datacenter. In
the year 2015 Cern reported 5500 nodes being used across two datacenters,
running more than 12000 virtual machines \cite{cern}. World's largest
telecommunication company China mobile has published multiple scalability tests
with OpenStack clusters of 1000 and more nodes \cite{china-mobile}. One of the
world's largest retail companies, Walmart invested in an OpenStack cloud
consisting of more than 100000 CPU cores \cite{walmart}.

Various approaches has been taken to cloud administration and update process by
different organizations. Cern for instance has traditionally followed a model
where one item is upgraded roughly every two weeks \cite{cern}, minimizing the
impact of potential failures.

Different parts of installation process has been automated as much as possible.
OpenStack services may be installed with package management tools, such as pip
for python source installation. Many Linux distributions' package management
tools, such as apt for Ubuntu and yum for CentOS, have distributions of many
OpenStack services as well, altough depending on the used repository,
distribution packages might not be at the latest versions.

Software configuration management tools have also been used by OpenStack
community for some time now. Many of the deployment methods for automation
tools have official OpenStack repositories. Some deployment methods are also
designed for particular use cases, such as easy setup of development
environment or automation testing.

\section{Deployment Methods}

OpenStack includes official repositories for many of the popular software
configuration management tools and some methods developed by the OpenStack
community. Methods have been developed at different times and for the most part
they have followed the best practices of the underlying configuration tool
whenever one is used.

Many deployment methods may have been developed with a particular use case in
mind. Some methods provide simple setup for an OpenStack development
environment, while others provide full configurability with minimal assumptions
of the deployment environment.

Currently many new developers being introduced to OpenStack start with
automated installation of the services. Being able to install OpenStack
proof-of-concept easily without knowledge of the automation tool makes
introduction to OpenStack easier thus increasing the potential size of the
developer community. OpenStack as many other open-source software projects is
partly developed by community contributions, so the ease of adaptation does
indirectly benefit the software development.

Simple setups for development environments are drastically easier to implement
than for production environments. Only practical requirement in development
environments is that the developed software is run and can be modified. Having
all of the applications deployed to the same host machine is most often
sufficient. This makes it possible to make accurate assumptions of the
deployment environment, and avoid manual cofigurations. Additionally there
exists a number of popular tools for development environments, such as Docker
or Vagrant. Setting up a development deployment can this way be reduced to
running a single command.

When comparing different deployment methods, this thesis focuses on production
environments. This means that the deployment should be configurable for the
needs of Service Level Agreements. Requirements for such environments may
include high availability, near-zero downtime, responsiveness, security, and
most importantly it has to be horizontally scalable.

Production-grade OpenStack clouds should first be designed based on their use
purposes without considering the use of automation. As OpenStack is a complex
set of services it can be used for various computing environments. Some clouds
focus on computational power, other maybe high network throughput or storage
volume. Once the architecture of the cloud is designed, any potential
automation tool should be decided based on how well they can be used for the
intended architecture. Cloud operators should also feel confortable with the
tool of choice as it will likely be used in many parts of the lifecycle of the
cloud.

In practice, when choosing an automation tool to be used for operating the
cloud, one important consideration is, how easy it is to find operators capable
of using the tool. This can be the most important factor for project managers,
as even the most optimal tool is not practical if there are no operators for
it. Many automation tools have evangelists that represent companies and make
the decision of the automation tool less of a technical consideration and more
of a business decision. An organization might end up selecting the tool that is
developed by a company whose other products are already in use. Often times
products developed by the same company have better support for
interoperability.

Even though using a family of software products from the same vendor has its
benefits, it can cause an architectural vendor-lock. In case one of the
products gets discontinued, replacing it with other solutions can become
difficult if other software solutions are already dependent of it. If the
product provider company gets aquired or goes down altogether, it may have a
significant effect on other organizations relying on their products or support.
One way to avoid vendor-lock is to use software products from various
organizations. This requires more effort and expertise than relying on the same
vendor. Alternatively similar products from various vendors can be used
simultaneously. This approach is likely cumbersome and potentially expensive.

Since OpenStack is supported by many organizations, there is a lot of different
alternatives for automation methods. Some of the methods are developed by
open-source communities, some by commercial organization. Some of the method
can be combined, or even have an official OpenStack project for compound
deployment method.

Some deployment methods suitable for OpenStack production environments are
introduced next. Methods will be approached from the perspective of the
research questions shown on Figure \ref{fig:rqs}. Dependencies for deployment
methods are displayed on Table \ref{tab:dependencies}.

Since all of the deployment methods listed below are open-source projects, they
can theoretically be modified for any configuration needs. However in order to
keep their comparisons reasonable, all of the required features should be
achieved with configuration instead of changes to the methods. In practice this
is not likely the case as deployment methods will likely be forked and modified
for custom needs. OpenStack community encourages merging any modifications to
the upstream project and avoiding the use of long-term forks. This unifies
practices in the OpenStack community and helps to develop the deployment
methods.

\begin{table}[t]
\centering
\begin{tabular}{ l }
\begin{tabular} {r | c c c } \\
              & TripleO     & OS Helm     & OS Ansible  \\
\hline
Provisioning  & OpenStack   & Kubernetes  & Bare metal  \\
Configuration & Puppet+Heat & Helm        & Ansible     \\
Runtime       & Nova/Ironic & Docker      & LXC/Bare    \\
\end{tabular} \\
\begin{tabular} {r | c c c } \\
              & Kolla Ansible & OS Charms   & OS Puppet \\
\hline
Provisioning  & Docker        & MAAS/Cloud  & Bare metal\\
Configuration & Ansible       & Juju        & Puppet    \\
Runtime       & Docker        & LXC/Bare    & Bare      \\
\end{tabular}
\end{tabular}
\caption{Deployment Method Dependencies}
\label{tab:dependencies}
\end{table}

\subsection{TripleO}

TripleO \cite{tripleo} is a method for deploying OpenStack by using another
OpenStack cloud for infrastructure provisioning and configuration. OpenStack
instance that is used for deployment by cloud administrators is called
'undercloud', and the instance being deployed for the end-users is called
'overcloud'.

Even though TripleO is largely based on OpenStack itself and no other software
configuration tools is needed, it has a steap learning curve due to the
complexity of resulting in two OpenStack deployments. On the other hand,
TripleO method benefits from prior knowledge of OpenStack, and it provides more
experience in using OpenStack services while designing and deploying overcloud.

After first installing OpenStack either manually or by using some of the other
automated deployment methods, TripleO can be used to deploy another OpenStack
instance by using OpenStack services to provision server machines. For a
performant deployment, Ironic Baremetal service is recommended instead of Nova.
This way the runtime environment of the overcloud is not using a virtual
machine.

TripleO uses Heat orchestration service to deploy overcloud as a stack. After
initial deployment Heat can be used to update overcloud by adding or removing
nodes, provided that undercloud has appropriate resources available. Nodes are
provisioned with Nova compute service with assistance of Ironic. Glance is used
for node machine images and Neutron for network provisioning.

Before deploying overcloud, undercloud must be prepared. In addition to
installing used OpenStack services, there are some preparation tasks for the
undercloud. Images used by the overcloud nodes must be added to Glance. When
using Ironic for baremetal provisioning, available nodes have to be registered
to the service. In a production-grade overcloud deployment, machine flavors
must also match hardware on target nodes.

Overcloud nodes are split into \textit{roles} that specify used image, flavor,
number of nodes with role, and Heat templates used for node configuration. In
addition to number of nodes with different roles, deployer can customize
overcloud OpenStack service configurations, network configuration and Ceph
storage cluster configuration. Otherwise TripleO deployment specifies much of
the overcloud features, as deployment is executed by OpenStack services running
on the undercloud.

\subsection{OpenStack Helm}

OpenStack Helm \cite{openstack-helm} is a collection of Helm Charts for
deploying OpenStack services into an existing Kubernetes cluster. Kubernetes
includes functionality for deploying, scaling, and upgrading OpenStack services
running in Docker containers while Helm charts describe components to
Kubernetes. Tradeoff with OpenStack Helm is the level of overhead for managing
a Kubernetes cluster for running Docker containers. This adds a potential for
ossifying Kubernetes into OpenStack environment.

Runtime environment, when using OpenStack Helms, depends on the Kubernetes
deployment. Kubernetes can be hosted by another cloud, but for production
environments, baremetal installation of Kubernetes is recommended. For setting
up a production-grade Kubernetes installation for OpenStack Helm, tools such as
Kubeadm or Airship \cite{airship} are recommended. However confguration of the
Kubernetes cluster is outside the scope of OpenStack Helm project
\cite{openstack-helm}.

OpenStack Helm includes Helm charts for deploying OpenStack services and the
needed infrastructure services. In order to customize service configurations,
these charts will have to be modified to suit deployment needs. Especially in
production use, charts will likely need to be modified \cite{openstack-helm}.
Doing so requires knowledge of both Helm and configured OpenStack services.

Since Kubernetes platform includes many configuration items and provides
RESTful API for users, there are questions from architectural point of view,
about the need to operate OpenStack cloud in Kubernetes. For operators familiar
with Kubernetes who want to provide OpenStack services, OpenStack Helm could be
an appropriate deployment method.

Kubernetes in general makes it easy to deploy changes in applications. It has a
number of available methods for application updates, such as rolling updates
and canary deployments. Many of these methods are beneficial for applications
that change rapidly while being actively used. OpenStack services need to be
updated twice a year at most, and in general they do not require zero-downtime,
as infrastructure provisioning for the most part can wait for hours as long as
the existing resources stay functional.

\subsection{OpenStack Ansible}

OpenStack Ansible \cite{openstack-ansible} uses Ansible roles created by
OpenStack community for deploying OpenStack. It is a low level deployment
method and requires good understanding of target environment and Ansible.
OpenStack Ansible was largely developed by Rackspace in the year 2014, but has
received community contributions later as well.

OpenStack Ansible does not include high level server provisioning as it is
intended to be used for pre-existing server infrastructure. This especially
makes adaptation of the method difficult as the infrastructure has to be
provided separately.

OpenStack Ansible repository includes an all-in-one deployment method for a
proof-of-concept installation on a single host. This method is not intended for
production as any further adjustments, such as horizontable scaling would
require drastic changes to the infrastructure.

As opposed to running services in Docker containers, like with OpenStack Helm
or Kolla Ansible, services can be installed into Linux containers
\cite{linuxcontainers}. Some OpenStack service processes, however are run as
native SystemD services for performance optimization.

OpenStack Ansible configuration enables deployed applications to be split to a
number of different node groups. This model supports various different
OpenStack architectures but determines some of the applications that will be
running on the same nodes. For the most part the divisions are sensible for
instance having infrastructure services in the same node group.

OpenStack Ansible is a relatively old deployment method, but it has had a
consistent user base.

\subsection{Kolla Ansible}

Kolla Ansible \cite{kolla-ansible} method uses Ansible to deploy OpenStack
services into Docker containers built with Kolla \cite{kolla}. Kolla is an
official OpenStack project for building production-ready Docker containers for
OpenStack services. Even though Kolla project is not dependent of Ansible,
Kolla Ansible is the primary deployment method used for containers built with
Kolla.

OpenStack services will be deployed in Docker containers without orchestration
tool such as Kubernetes. While avoiding the need to maintain a Kubernetes
cluster, managing Docker containers manually results in more low-level
administrative tasks related to scaling and updating the deployment.

Kolla Ansible is a popular deployment method for developers getting introduced
to OpenStack as it provides a simple all-in-one setup for OpenStack. For
developers already familiar with Docker, Kolla Ansible is relatively easy to
get into.

Another benefit of using Kolla Ansible project as opposed to deploying
OpenStack with Ansible to LXC containers, is to leverage the functionality of
Docker Engine for virtual resource provisioning. Docker provides simple
commands for provisioning networks and volumes in a host-agnostic way. With
native virtualization tools, device provisioning is typically done by different
programs, such as LVM for volumes and bridgeutils for virtual network
interfaces.

With a low-level containerization tool such as LXC, virtual networks for
containers have to be created manually. This includes creating veth pairs, and
bridging appropriate interfaces to wanted physical network interfaces. These
operations typically require an operator who is capable with networking. For
this reason, Docker provides simple commands for setting up virtual networks,
and a variety of different network types. Even overlay networks accross
multiple host operating systems can be setup with Docker.

In addition to provisioning virtual devices, docker provides other commands for
operating on them in a simple fashion. Devices are easy to attach or detach
from containers. Simplicity of use and reliability of operations is one of the
reasons for Docker popularity.

According to user-surveys container-based deployment methods have recenlty
become popular among OpenStack community. This follows the trend of containers
becoming generally popular in software development and administration. Having
container based method for environment setup will likely bring more potential
developers to OpenStack community.

\subsection{OpenStack Charms}

OpenStack Charms \cite{charm-deployment-guide} method uses Juju for deployment
of OpenStack. Due to OpenStack Charms deployment method being largely
maintained by Canonical, it has a relatively large support from Canonical but
at the same time it has a potential for vendor-lock. OpenStack Charms only
supports Ubuntu target hosts. On the other hand, according to OpenStack user
surveys, Ubuntu is the most commonly used host distro in OpenStack deployments.
Likewise Juju is a popular method for both developers getting introduced to
OpenStack and OpenStack administrators.

OpenStack Charms is a versatile deployment method that is applicable for both
OpenStack development and production use. It supports a variety of existing
cloud providers. Once developer or operator is familiar Juju, it can be used
for easy setup of a number of other services as well.

Canonical recommends MAAS to be used for datacenter provisioning in a
production OpenStack deployment. A large part of the Juju functionality comes
from cloud controllers, manual provisioning is not recommended even though it
is supported with Juju as well. MAAS provides lightweight baremetal server
provisioning has a good support for use with Juju.

OpenStack Charms is a deployment method that includes a lot of software
developed by Canonical. It is a good example of a set of software solutions
from a single vendor that provide a good support for each other. Canonical also
provides consulting and exerice for custom OpenStack cloud builds.  OpenStack
marketplace includes also tailored OpenStack software solutions with decent
support. One reason for using Canonical's support for building OpenStack cloud
with OpenStack Charms could be to get an open-source installation of OpenStack
on-premises and get mentoring for data center administrators in the process.

From a techincal perspective one of the reasons why developers may like Juju
and as a result OpenStack Charms deployment method, is the fact that the
configuration operations are defined with application modules. Even though many
of the configuration management tools and OpenStack deployment methods use
application modules for low-level tasks, they often times are configured in a
declarative language such as YAML. While this makes configurations
understandable at high level, the concrete configuration tasks are hidden in
the application modules that often times are included in separate source
repositories.

\subsection{Puppet OpenStack}

Puppet OpenStack project houses Puppet \cite{puppet-deployment-guide} modules
for deploying OpenStack services. Additionally it includes scipts to setup
deployments including an all-in-one proof of concept setup. In order to use
Puppet OpenStack, Puppet needs to be installed in data center.

Puppet OpenStack is largely used for developmnt environment setups, and for
automated testing of OpenStack services. Additionally it can be used for
large-scale deployments of OpenStack. Puppet OpenStack consists of Puppet
modules for each of the OpenStack components. Each of the modules has its own
source repository.

\begin{thebibliography}{9}

\bibitem{aws}
Amazon Web Services (AWS), [Online]. \\
Available: \textit{https://aws.amazon.com/}

\bibitem{azure}
Microsoft Azure, [Online]. \\
Available: \textit{https://azure.microsoft.com/}

\bibitem{gcp}
Google Cloud Platform, [Online]. \\
Available \textit{https://cloud.google.com/}

\bibitem{mapreduce}
J. Dean et al:
\\\href{https://dl.acm.org/doi/abs/10.1145/1327452.1327492}
{MapReduce: simplified data processing on large clusters},
Communications of ACM, Jan. 2008

\bibitem{openstack}
O. Sefraoui et al:
\\\href{https://pdfs.semanticscholar.org/4be2/28917846a218ba00d30b42d709a11b7a5311.pdf}
{OpenStack: Toward an Open-Source Solution for Cloud Computing},
International Journal of Computer Applications, vol. 55, no. 3, pp. 81-84, Oct.
2012

\bibitem{forrester}
Paul Miller, Lauren E. Nelson:
\\\href{https://object-storage-ca-ymq-1.vexxhost.net/swift/v1/6e4619c416ff4bd19e1c087f27a43eea/www-assets-prod/pdf-downloads/Brief-OpenStack-Is-Now-Ready.pdf}{Brief: OpenStack Is Now Ready For Business}

\bibitem{cgroup}
Linux Manuals: CGroups, [Online]. \\
Available: \textit{https://man7.org/linux/man-pages/man7/cgroups.7.html}

\bibitem{docker}
D. Bernstein:
\\\href{https://ieeexplore.ieee.org/abstract/document/7036275}{Containers and
Cloud: From LXC to Docker to Kubernetes},
IEEE Cloud Computing, vol. 1, no. 3, pp. 81-84, Sept. 2014

\bibitem{kubernetes}
B Burns et al:
\\\href{https://dl.acm.org/doi/pdf/10.1145/2898442.2898444}{Borg, Omega, and
Kubernetes},
Queue, 2016

\bibitem{json}
JSON, [Online]. \\
Available: \textit{https://www.json.org/}

\bibitem{yaml}
YAML, [Online]. \\
Available: \textit{https://yaml.org/}

\bibitem{ansible}
N Singh et al:
\\\href{https://ieeexplore.ieee.org/abstract/document/7375087}{Automated
provisioning of application in IAAS cloud using Ansible configuration management},
International Conference on Next Generation Computing Technologies (NGCT)
Dehradun, 2015

\bibitem{juju}
Juju Charms, [Online]. \\
Available: \textit{https://jujucharms.com/}

\bibitem{maas}
A Sirbu et al:
\\\href{https://dl.acm.org/doi/abs/10.1145/2747470.2747473}{MaaS advanced
provisioning and reservation system},
In Proceedings of the 1st International Workshop on Automated Incident
Management in Cloud. Association for Computing Machinery, New York, 2015

\bibitem{puppet}
Puppet, [Online]. \\
Available: \textit{https://github.com/puppetlabs/puppet/}

\bibitem{cern}
T Bell et al:
\\\href{https://iopscience.iop.org/article/10.1088/1742-6596/664/2/022003/pdf}{
Scaling the CERN OpenStack cloud},
J. Phys.: Conf. Ser. 664 022003, 2015

\bibitem{china-mobile}
Yingching Cheng, Malini Bhandaru, Junwei Liu:
\\\href{https://01.org/sites/default/files/performance_analysis_and_tuning_in_china_mobiles_openstack_production_cloud_2.pdf}{Analysing and Tuning China Mobile's OpenStack Production Cloud}

\bibitem{walmart}
Ali Kanso, Nicolas Deixionne, Abdelouahed Gherbi, Feredoun Farrahi Moghaddam:
\\\href{https://ieeexplore.ieee.org/abstract/document/7911874}{Enhancing OpenStack Fault Tolerance for Provisioning Computing Environments}

\bibitem{tripleo}
TripleO, [Online]. \\
Available: \textit{https://docs.openstack.org/tripleo-docs/latest/}

\bibitem{openstack-helm}
OpenStack Helm, [Online]. \\
Available: \textit{https://docs.openstack.org/openstack-helm/latest/}

\bibitem{airship}
Airship, [Online]. \\
Available: \textit{https://www.airshipit.org/}

\bibitem{openstack-ansible}
OpenStack Ansible, [Online]. \\
Available: \textit{https://opendev.org/openstack/openstack-ansible/}

\bibitem{linuxcontainers}
Linux Containers, [Online]. \\
Available: \textit{https://linuxcontainers.org/}

\bibitem{kolla-ansible}
Kolla Ansible, [Online]. \\
Available: \textit{https://opendev.org/openstack/kolla-ansible/}

\bibitem{kolla}
Kolla, [Online]. \\
Available: \textit{https://opendev.org/openstack/kolla/}

\bibitem{charm-deployment-guide}
OpenStack Charms Deployment Guilde, [Online]. \\
Available: \textit{https://opendev.org/openstack/charm-deployment-guide/}

\bibitem{puppet-deployment-guide}
OpenStack Puppet Deployment Guilde, [Online]. \\
Available: \textit{https://docs.openstack.org/puppet-openstack-guide/latest/}

\end{thebibliography}

\end{document}
